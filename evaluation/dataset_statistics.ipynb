{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Calculating Data Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages and Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from utilities.utils import load_json\n",
    "from utilities.preprocess import preprocess_patient_state_tuples\n",
    "\n",
    "def describe_emrs(emrs: List[str], tokenizer: AutoTokenizer) -> None:\n",
    "    # Number of words, tokens, and characters\n",
    "    # words (tokenized by whitespace)\n",
    "    nwords = list()\n",
    "    for emr in emrs:\n",
    "        nword = len(emr.split())\n",
    "        nwords.append(nword)\n",
    "    nwords = np.array(nwords)\n",
    "\n",
    "    # tokens\n",
    "    tokenized_emrs = tokenizer(emrs)[\"input_ids\"]\n",
    "    ntokens = np.array(list(map(lambda l: len(l), tokenized_emrs)))\n",
    "\n",
    "    # characters\n",
    "    nchars = list()\n",
    "    for emr in emrs:\n",
    "        nchar = len(emr)\n",
    "        nchars.append(nchar)\n",
    "    nchars = np.array(nchars)\n",
    "\n",
    "    for item, name in zip([nwords, ntokens, nchars], [\"words\", \"tokens\", \"chars\"]):\n",
    "        print(f\"Number of {name}: {pd.Series(item).describe()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full EMRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data & tokenizer\n",
    "full_emrs = load_json(\"../datasets/full_piphs.json\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../models/ner/tokenizer/\", use_fast=True)\n",
    "\n",
    "# Print data statistics\n",
    "describe_emrs(full_emrs, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracted Findings (Unnormalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data & tokenizer\n",
    "unnorm_emrs = load_json(\"../datasets/unnorm_patient_states_t.json\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../models/ner/tokenizer/\", use_fast=True)\n",
    "\n",
    "# preprocessing\n",
    "label2token = {\n",
    "    0: \"positive\",\n",
    "    1: \"negative\"\n",
    "}\n",
    "ppsd_unnorm_emrs = preprocess_patient_state_tuples(state_tuples_l=unnorm_emrs, label2token=label2token)\n",
    "\n",
    "# Print data statistics\n",
    "describe_emrs(ppsd_unnorm_emrs, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracted Findings (Normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data & tokenizer\n",
    "norm_emrs = load_json(\"../datasets/norm_patient_states_t.json\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../models/ner/tokenizer/\", use_fast=True)\n",
    "\n",
    "# preprocessing\n",
    "label2token = {\n",
    "    0: \"positive\",\n",
    "    1: \"negative\"\n",
    "}\n",
    "ppsd_norm_emrs = preprocess_patient_state_tuples(state_tuples_l=norm_emrs, label2token=label2token)\n",
    "\n",
    "# Print data statistics\n",
    "describe_emrs(ppsd_norm_emrs, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cuda-11.3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35b66dd0c8f752918e1728d86abaa8fb004a7dee1d90779ea4d0023d852f9fe7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
