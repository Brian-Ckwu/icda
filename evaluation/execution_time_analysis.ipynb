{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from argparse import Namespace\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "from utilities.preprocess import train_valid_test_split, pad_int_icd\n",
    "from utilities.utils import load_json, set_seeds, build_reverse_dict\n",
    "from utilities.model import BertNERModel, BiEncoder, BertDxModel, encoder_names_mapping\n",
    "\n",
    "from icda import ICDA\n",
    "from finding_extractor import FindingExtractor, Recognizer, Normalizer\n",
    "from diagnosis_classifier import DiagnosisClassifier\n",
    "from term_suggester import TermSuggester, UMLSClassifier\n",
    "from emr_preprocessor import EMRPreprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    full_emr_path=\"../datasets/notes_B_full.json\",\n",
    "    unnorm_states_path=\"../datasets/notes_B_unnorm.json\",\n",
    "    norm_states_path=\"../datasets/notes_B_norm.json\",\n",
    "    in_icds_path=\"../datasets/in_icds.json\",\n",
    "    out_icds_path=\"../datasets/out_icds.json\",\n",
    "\n",
    "    ner_model_path=\"../models/ner\",\n",
    "    batch_size=16,\n",
    "\n",
    "    nen_model_path=\"../models/nen\",\n",
    "\n",
    "    dx_model_path=\"../models/dx\",\n",
    "    target_metric=\"hat5\",\n",
    "\n",
    "    score_matrix_path=\"../models/term/fisher_matrix_mink-3_minp-0.05.csv\",\n",
    "    term2id_path=\"../models/term/term2id.json\",\n",
    "    inequality=\"lesser\",\n",
    "    threshold=0.05,\n",
    "    ndx=5,\n",
    "\n",
    "    seed=7,\n",
    "    train_size=0.8,\n",
    "    valid_size=0.1,\n",
    "    test_size=0.1,\n",
    "\n",
    "    system_mode=\"deploy\",\n",
    "    extract_mode=\"umls\",\n",
    "    front_end=\"unified\",\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "\n",
    "set_seeds(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ICDA Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "ner_model = BertNERModel(encoder=encoder_names_mapping[\"BioLinkBERT\"], num_tags=5)\n",
    "ner_model.load_state_dict(torch.load(Path(args.ner_model_path) / \"best_model.pth\", map_location=args.device))\n",
    "ner_tokenizer = AutoTokenizer.from_pretrained(Path(args.ner_model_path) / \"tokenizer\", use_fast=True)\n",
    "\n",
    "nen_model = BiEncoder(encoder_name=encoder_names_mapping[\"BioLinkBERT\"])\n",
    "nen_model.load_state_dict(torch.load(Path(args.nen_model_path) / \"best_valid_acc.pth\", map_location=args.device))\n",
    "nen_tokenizer = AutoTokenizer.from_pretrained(Path(args.nen_model_path) / \"tokenizer\", use_fast=True)\n",
    "entity_embeddings = torch.load(Path(args.nen_model_path) / \"entity_embeddings.pt\")\n",
    "\n",
    "cui2name = load_json(Path(args.nen_model_path) / \"smcui2name.json\")\n",
    "cui2typeinfo = load_json(Path(args.nen_model_path) / \"smcui2typeinfo.json\")\n",
    "cat2typenames = load_json(Path(args.nen_model_path) / \"cat2typenames.json\")\n",
    "\n",
    "id2dx = load_json(Path(args.dx_model_path) / \"id2icd.json\")\n",
    "dx2name = load_json(Path(args.dx_model_path) / \"icdnine2name_en.json\")\n",
    "dx_model = BertDxModel(encoder_name=encoder_names_mapping[\"BioLinkBERT\"], num_dxs=len(id2dx))\n",
    "dx_model.load_state_dict(torch.load(Path(args.dx_model_path) / f\"best_{args.target_metric}.pth\"))\n",
    "dx_tokenizer = AutoTokenizer.from_pretrained(Path(args.ner_model_path) / \"tokenizer\", use_fast=True)\n",
    "\n",
    "fisher_matrix = pd.read_csv(args.score_matrix_path, index_col=\"term_id\")\n",
    "term2id = load_json(args.term2id_path)\n",
    "id2term = build_reverse_dict(term2id)\n",
    "\n",
    "# Sub-components\n",
    "recognizer = Recognizer(\n",
    "    model=ner_model,\n",
    "    tokenizer=ner_tokenizer,\n",
    "    batch_size=args.batch_size,\n",
    "    device=args.device\n",
    ")\n",
    "\n",
    "normalizer = Normalizer(\n",
    "    model=nen_model,\n",
    "    tokenizer=nen_tokenizer,\n",
    "    entity_embeddings=entity_embeddings,\n",
    "    cui2name=cui2name,\n",
    "    device=args.device,\n",
    "    emr_batch_size=1,\n",
    "    cui_batch_size=args.batch_size\n",
    ")\n",
    "\n",
    "umls_classifier = UMLSClassifier(\n",
    "    cui2name=cui2name,\n",
    "    cui2typeinfo=cui2typeinfo,\n",
    "    cat2typenames=cat2typenames\n",
    ")\n",
    "\n",
    "# Components\n",
    "finding_extractor = FindingExtractor(\n",
    "    recognizer=recognizer,\n",
    "    normalizer=normalizer\n",
    ")\n",
    "\n",
    "emr_preprocessor = EMRPreprocessor(\n",
    "    finding_extractor=finding_extractor\n",
    ")\n",
    "\n",
    "dx_classifier = DiagnosisClassifier(\n",
    "    model=dx_model,\n",
    "    tokenizer=dx_tokenizer,\n",
    "    id2dx=id2dx,\n",
    "    dx2name=dx2name,\n",
    "    batch_size=args.batch_size,\n",
    "    device=args.device\n",
    ")\n",
    "\n",
    "term_suggester = TermSuggester(\n",
    "    score_matrix=fisher_matrix,\n",
    "    id2term=id2term,\n",
    "    inequality=args.inequality,\n",
    "    threshold=args.threshold,\n",
    "    diagnosis_classifier=dx_classifier,\n",
    "    umls_classifier=umls_classifier,\n",
    "    top_k_dxs=args.ndx\n",
    ")\n",
    "\n",
    "icda = ICDA(\n",
    "    system_mode=args.system_mode,\n",
    "    extract_mode=args.extract_mode,\n",
    "    front_end=args.front_end,\n",
    "    finding_extractor=finding_extractor,\n",
    "    diagnosis_classifier=dx_classifier,\n",
    "    term_suggester=term_suggester,\n",
    "    emr_preprocessor=emr_preprocessor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_l = load_json(args.full_emr_path)\n",
    "out_icds = load_json(args.out_icds_path)\n",
    "\n",
    "id2icd = load_json(Path(args.dx_model_path) / \"id2icd.json\")\n",
    "icd2id = {pad_int_icd(icd): int(id_) for id_, icd in id2icd.items()}\n",
    "labels = [icd2id[icd] for icd in out_icds]\n",
    "\n",
    "train_inputs, valid_inputs, test_inputs, train_outs, valid_outs, test_outs = train_valid_test_split(\n",
    "    inputs=text_l,\n",
    "    labels=labels,\n",
    "    train_size=args.train_size,\n",
    "    valid_size=args.valid_size,\n",
    "    test_size=args.valid_size,\n",
    "    seed=args.seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instance-by-Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def time_it(func, *args, **kwargs):\n",
    "    start = time.time()\n",
    "    func(*args, **kwargs)\n",
    "    end = time.time()\n",
    "    return end - start\n",
    "\n",
    "times = list()\n",
    "\n",
    "for text in tqdm(test_inputs):\n",
    "    t = time_it(icda.generate_support, [text], n_dx=5)\n",
    "    times.append(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "batch_times = list()\n",
    "\n",
    "for i in tqdm(range(0, len(test_inputs), batch_size)):\n",
    "    input_l = test_inputs[i:i + batch_size]\n",
    "    batch_t = time_it(icda.generate_support, input_l, n_dx=5)\n",
    "    batch_times.append(batch_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 500\n",
    "\n",
    "times_df = pd.DataFrame(times).rename({0: \"Execution time (seconds)\"}, axis=1)\n",
    "\n",
    "sns.histplot(times_df, x=\"Execution time (seconds)\", bins=40, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(times).describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cuda-11.3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35b66dd0c8f752918e1728d86abaa8fb004a7dee1d90779ea4d0023d852f9fe7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
