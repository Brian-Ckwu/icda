{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import softmax\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "from utilities.preprocess import preprocess_patient_state_tuples, train_valid_test_split, pad_int_icd\n",
    "from utilities.utils import load_json, save_json, set_seeds, build_reverse_dict, move_bert_input_to_device\n",
    "from utilities.model import BertNERModel, BiEncoder, BertDxModel, encoder_names_mapping\n",
    "from utilities.data import MedicalDxDataset\n",
    "from utilities.term import build_term_ids_lists\n",
    "\n",
    "from icda import ICDA\n",
    "from finding_extractor import FindingExtractor, Recognizer, Normalizer\n",
    "from diagnosis_classifier import DiagnosisClassifier\n",
    "from term_suggester import TermSuggester\n",
    "from emr_preprocessor import EMRPreprocessor\n",
    "from state_tracker import PatientStateTracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_incremental_states(states):\n",
    "    docs = list()\n",
    "\n",
    "    for doc in states:\n",
    "        subdocs = list()\n",
    "        for i in range(1, len(doc) + 1):\n",
    "            subdoc = list()\n",
    "            for j in range(i):\n",
    "                concept = doc[j][0]\n",
    "                pol = doc[j][1]\n",
    "                pol_name = \"positive\" if (pol == 0) else \"negative\"\n",
    "                subdoc += [pol_name, concept]\n",
    "            subdoc = ' '.join(subdoc)\n",
    "            subdocs.append(subdoc)\n",
    "        if not subdocs:\n",
    "            subdocs.append(\"\")\n",
    "        docs.append(subdocs)\n",
    "    \n",
    "    return docs\n",
    "\n",
    "def calc_all_cnfds(model: BertDxModel, tokenizer: AutoTokenizer, docs: List[List[str]], ys: List[int]):\n",
    "    all_cnfds = list()\n",
    "\n",
    "    for subdocs, y in tqdm(zip(docs, ys)):\n",
    "        valid_set = MedicalDxDataset(emrs=subdocs, dx_labels=[y] * len(subdocs), tokenizer=tokenizer)\n",
    "        valid_loader = DataLoader(valid_set, batch_size=16, shuffle=False, pin_memory=True, collate_fn=valid_set.collate_fn)\n",
    "\n",
    "        model.eval()\n",
    "        all_logits = list()\n",
    "        for X, _ in valid_loader:\n",
    "            X = move_bert_input_to_device(X, \"cuda\")\n",
    "            with torch.no_grad():\n",
    "                logits = model(X)\n",
    "                all_logits.append(logits)\n",
    "        \n",
    "        all_logits = torch.cat(all_logits, dim=0)\n",
    "        cnfds = softmax(all_logits, dim=-1)[:, y].cpu().tolist()\n",
    "        all_cnfds.append(cnfds)\n",
    "    \n",
    "    return all_cnfds\n",
    "\n",
    "def select_states_gt_threshold(states, labels, cnfds_l, th_low: float) -> tuple:\n",
    "    assert len(states) == len(labels) == len(cnfds_l)\n",
    "    sel_states = list()\n",
    "    sel_labels = list()\n",
    "    sel_cnfds_l = list()\n",
    "    # select states where there is a state_obs where cnfd > th_low\n",
    "    for state, label, cnfds in zip(states, labels, cnfds_l):\n",
    "        if state:\n",
    "            assert len(state) == len(cnfds)\n",
    "            for i, cnfd in enumerate(cnfds):\n",
    "                if cnfd > th_low:\n",
    "                    state_obs = state[:i + 1]\n",
    "                    state_rem = state[i + 1:]\n",
    "                    sel_states.append({\n",
    "                        \"obs\": state_obs,\n",
    "                        \"rem\": state_rem\n",
    "                    })\n",
    "                    sel_labels.append(label)\n",
    "                    sel_cnfds_l.append(cnfds)\n",
    "                    break\n",
    "\n",
    "    return sel_states, sel_labels, sel_cnfds_l\n",
    "\n",
    "def static_rank_rem(obs, new_rem, label, icda: ICDA):\n",
    "    new_rem_with_score = list()\n",
    "    for term, pol in new_rem:\n",
    "        term_id = icda.term_suggester.term2id[term]\n",
    "        score = icda.term_suggester.score_matrix.at[term_id, label]\n",
    "        new_rem_with_score.append(([term, pol], score))\n",
    "    \n",
    "    ranked_new_rem_with_score = sorted(new_rem_with_score, key=lambda t: t[1])\n",
    "    ranked_new_rem = [term_pol for term_pol, _ in ranked_new_rem_with_score]\n",
    "    return ranked_new_rem\n",
    "\n",
    "def dynamic_rank_rem(obs, new_rem, label, icda: ICDA):\n",
    "    obs = obs.copy()\n",
    "    ranked_new_rem = list()\n",
    "    while len(new_rem) > 0:\n",
    "        rem_with_score = list()\n",
    "\n",
    "        obs_plus_term_l = list()\n",
    "        for term_pol in new_rem:\n",
    "            term = term_pol[0]\n",
    "            obs_plus_term = obs + [[term, 0]]\n",
    "            obs_plus_term_l.append(obs_plus_term)\n",
    "\n",
    "        obs_plus_term_text_l = preprocess_patient_state_tuples(obs_plus_term_l, label2token={0: \"positive\", 1: \"negative\"})\n",
    "        # predict diagnosis\n",
    "        all_logits = icda.diagnosis_classifier.predict(obs_plus_term_text_l)\n",
    "        dxs_l, probs_l = icda.diagnosis_classifier.get_top_dxs_with_probs(all_logits, top_k=113)\n",
    "        \n",
    "        assert len(dxs_l) == len(probs_l) == len(new_rem)\n",
    "        for dxs, probs, term_pol in zip(dxs_l, probs_l, new_rem):\n",
    "            dx_idx = dxs.index(label)\n",
    "            score = probs[dx_idx]\n",
    "            # append tuple\n",
    "            rem_with_score.append((term_pol, score))\n",
    "        \n",
    "        ranked_rem_with_score = sorted(rem_with_score, key=lambda t: t[1], reverse=True)\n",
    "        top_term_pol, score = ranked_rem_with_score[0]\n",
    "        obs.append(top_term_pol)\n",
    "        ranked_new_rem.append(top_term_pol)\n",
    "        new_rem.remove(top_term_pol)\n",
    "\n",
    "    return ranked_new_rem\n",
    "\n",
    "def suggest_term_by_scheme(sel_states, sel_labels, icda: ICDA, scheme: str):\n",
    "    assert len(sel_states) == len(sel_labels)\n",
    "    if scheme not in [\"random\", \"physician\", \"static\", \"dynamic\"]:\n",
    "        raise ValueError()\n",
    "    \n",
    "    rem_states = list()\n",
    "    for state, label in tqdm(zip(sel_states, sel_labels)):\n",
    "        obs = state[\"obs\"]\n",
    "        rem = state[\"rem\"]\n",
    "        new_rem = rem.copy()\n",
    "        # rearrangement\n",
    "        if scheme == \"random\":\n",
    "            random.shuffle(new_rem)\n",
    "        elif scheme == \"static\":\n",
    "            new_rem = static_rank_rem(obs, new_rem, label, icda)\n",
    "        elif scheme == \"dynamic\":\n",
    "            new_rem = dynamic_rank_rem(obs, new_rem, label, icda)\n",
    "        \n",
    "        rem_states.append(new_rem)\n",
    "    \n",
    "    return rem_states\n",
    "\n",
    "def calc_cnfds_l_from_states(obs_states, rem_states, dx_model, tokenizer, labels):\n",
    "    assert len(obs_states) == len(rem_states) == len(labels)\n",
    "    full_states = list()\n",
    "    for obs, rem in zip(obs_states, rem_states):\n",
    "        full = obs + rem\n",
    "        full_states.append(full)\n",
    "    incremental_docs = make_incremental_states(states=full_states)\n",
    "    cnfds_l = calc_all_cnfds(model=dx_model, tokenizer=tokenizer, docs=incremental_docs, ys=labels)\n",
    "\n",
    "    return cnfds_l\n",
    "\n",
    "def calc_term_usage(obs_states, cnfds_l, th_highs: List[float], default_upper_limit: int = 10):\n",
    "    assert len(obs_states) == len(cnfds_l)\n",
    "    \n",
    "    term_usage = {th_high: list() for th_high in th_highs}\n",
    "    for obs_state, cnfds in zip(obs_states, cnfds_l):\n",
    "        start_idx = len(obs_state)\n",
    "        rem_cnfds = cnfds[start_idx:]\n",
    "        for th in th_highs:\n",
    "            exceed_th = False\n",
    "            for i, cnfd in enumerate(rem_cnfds):\n",
    "                nterms = i + 1\n",
    "                if nterms >= default_upper_limit:\n",
    "                    break\n",
    "                if cnfd > th:\n",
    "                    term_usage[th].append(nterms)\n",
    "                    exceed_th = True\n",
    "                    break\n",
    "            if not exceed_th:\n",
    "                term_usage[th].append(default_upper_limit)\n",
    "\n",
    "    for term_counts in term_usage.values():\n",
    "        assert len(term_counts) == len(cnfds_l)\n",
    "\n",
    "    return {th: pd.Series(term_usage[th]).describe() for th in th_highs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    full_emr_path=\"../../datasets/notes_B_full.json\",\n",
    "    unnorm_states_path=\"../../datasets/notes_B_unnorm.json\",\n",
    "    norm_states_path=\"../../datasets/notes_B_norm.json\",\n",
    "    in_icds_path=\"../../datasets/in_icds.json\",\n",
    "    out_icds_path=\"../../datasets/out_icds.json\",\n",
    "\n",
    "    ner_model_path=\"../../models/ner\",\n",
    "    batch_size=16,\n",
    "\n",
    "    nen_model_path=\"../../models/nen\",\n",
    "\n",
    "    dx_model_path=\"../../models/dx\",\n",
    "    target_metric=\"hat5\",\n",
    "\n",
    "    score_matrix_path=\"../../models/term/fisher_matrix_mink-3_minp-0.05.csv\",\n",
    "    term2id_path=\"../../models/term/term2id.json\",\n",
    "    inequality=\"lesser\",\n",
    "    threshold=0.10,\n",
    "    ndx=5,\n",
    "\n",
    "    seed=7,\n",
    "    train_size=0.8,\n",
    "    valid_size=0.1,\n",
    "    test_size=0.1,\n",
    "\n",
    "    system_mode=\"test\",\n",
    "    extract_mode=\"umls\",\n",
    "    front_end=\"unified\",\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "\n",
    "set_seeds(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "norm_states = load_json(args.norm_states_path)\n",
    "in_icds = load_json(args.in_icds_path)\n",
    "out_icds = load_json(args.out_icds_path)\n",
    "\n",
    "id2icd = load_json(Path(args.dx_model_path) / \"id2icd.json\")\n",
    "icd2id = {pad_int_icd(icd): int(id_) for id_, icd in id2icd.items()}\n",
    "labels = [icd2id[icd] for icd in out_icds]\n",
    "\n",
    "# Split data\n",
    "train_inputs, valid_inputs, test_inputs, train_outs, valid_outs, test_outs = train_valid_test_split(\n",
    "    inputs=norm_states,\n",
    "    labels=out_icds,\n",
    "    train_size=args.train_size,\n",
    "    valid_size=args.valid_size,\n",
    "    test_size=args.valid_size,\n",
    "    seed=args.seed\n",
    ")\n",
    "\n",
    "train_ins, valid_ins, test_ins, _, _, _ = train_valid_test_split(\n",
    "    inputs=in_icds,\n",
    "    labels=out_icds,\n",
    "    train_size=args.train_size,\n",
    "    valid_size=args.valid_size,\n",
    "    test_size=args.valid_size,\n",
    "    seed=args.seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "ner_model = BertNERModel(encoder=encoder_names_mapping[\"BioLinkBERT\"], num_tags=5)\n",
    "ner_model.load_state_dict(torch.load(Path(args.ner_model_path) / \"best_model.pth\", map_location=args.device))\n",
    "ner_tokenizer = AutoTokenizer.from_pretrained(Path(args.ner_model_path) / \"tokenizer\", use_fast=True)\n",
    "\n",
    "nen_model = BiEncoder(encoder_name=encoder_names_mapping[\"BioLinkBERT\"])\n",
    "nen_model.load_state_dict(torch.load(Path(args.nen_model_path) / \"best_valid_acc.pth\", map_location=args.device))\n",
    "nen_tokenizer = AutoTokenizer.from_pretrained(Path(args.nen_model_path) / \"tokenizer\", use_fast=True)\n",
    "entity_embeddings = torch.load(Path(args.nen_model_path) / \"entity_embeddings.pt\")\n",
    "cui2name = load_json(Path(args.nen_model_path) / \"smcui2name.json\")\n",
    "\n",
    "dx_model = BertDxModel(encoder_name=encoder_names_mapping[\"BioLinkBERT\"], num_dxs=len(Counter(out_icds)))\n",
    "dx_model.load_state_dict(torch.load(Path(args.dx_model_path) / f\"best_{args.target_metric}.pth\"))\n",
    "dx_tokenizer = AutoTokenizer.from_pretrained(Path(args.ner_model_path) / \"tokenizer\", use_fast=True)\n",
    "id2dx = load_json(Path(args.dx_model_path) / \"id2icd.json\")\n",
    "dx2name = load_json(Path(\"../../models/dx\") / \"icdnine2name_en.json\")\n",
    "\n",
    "fisher_matrix = pd.read_csv(args.score_matrix_path, index_col=\"term_id\")\n",
    "term2id = load_json(args.term2id_path)\n",
    "id2term = build_reverse_dict(term2id)\n",
    "\n",
    "# Components\n",
    "recognizer = Recognizer(\n",
    "    model=ner_model,\n",
    "    tokenizer=ner_tokenizer,\n",
    "    batch_size=args.batch_size,\n",
    "    device=args.device\n",
    ")\n",
    "\n",
    "normalizer = Normalizer(\n",
    "    model=nen_model,\n",
    "    tokenizer=nen_tokenizer,\n",
    "    entity_embeddings=entity_embeddings,\n",
    "    cui2name=cui2name,\n",
    "    device=args.device,\n",
    "    emr_batch_size=1,\n",
    "    cui_batch_size=args.batch_size\n",
    ")\n",
    "\n",
    "finding_extractor = FindingExtractor(\n",
    "    recognizer=recognizer,\n",
    "    normalizer=normalizer\n",
    ")\n",
    "\n",
    "emr_preprocessor = EMRPreprocessor(\n",
    "    finding_extractor=finding_extractor\n",
    ")\n",
    "\n",
    "dx_classifier = DiagnosisClassifier(\n",
    "    model=dx_model,\n",
    "    tokenizer=dx_tokenizer,\n",
    "    id2dx=id2dx,\n",
    "    dx2name=dx2name,\n",
    "    batch_size=args.batch_size,\n",
    "    device=args.device\n",
    ")\n",
    "\n",
    "term_suggester = TermSuggester(\n",
    "    score_matrix=fisher_matrix,\n",
    "    id2term=id2term,\n",
    "    inequality=args.inequality,\n",
    "    threshold=args.threshold,\n",
    "    diagnosis_classifier=dx_classifier,\n",
    "    umls_classifier=None,\n",
    "    top_k_dxs=args.ndx\n",
    ")\n",
    "\n",
    "icda = ICDA(\n",
    "    system_mode=args.system_mode,\n",
    "    extract_mode=args.extract_mode,\n",
    "    front_end=args.front_end,\n",
    "    finding_extractor=finding_extractor,\n",
    "    diagnosis_classifier=dx_classifier,\n",
    "    term_suggester=term_suggester,\n",
    "    emr_preprocessor=emr_preprocessor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx_model = dx_model.to(args.device)\n",
    "\n",
    "# Estimate model confidence on the incremental states of test set notes\n",
    "physician_incremental_docs = make_incremental_states(states=test_inputs)\n",
    "all_cnfds = calc_all_cnfds(model=dx_model, tokenizer=ner_tokenizer, docs=physician_incremental_docs, ys=[dx_classifier.dx2id[dx] for dx in test_outs])\n",
    "\n",
    "# Select notes where there are states of which model confidence of y_d >= lower threshold\n",
    "sel_states, sel_labels, sel_cnfds_l = select_states_gt_threshold(states=test_inputs, labels=test_outs, cnfds_l=all_cnfds, th_low=0.25)\n",
    "\n",
    "# Ranking approaches\n",
    "approaches = [\"random\", \"physician\", \"static\", \"dynamic\"]\n",
    "\n",
    "obs_states = [state[\"obs\"] for state in sel_states]\n",
    "\n",
    "# Suggest terms by different ranking approaches\n",
    "random_rem_states = suggest_term_by_scheme(sel_states, sel_labels, icda, scheme=\"random\")\n",
    "physician_rem_states = suggest_term_by_scheme(sel_states, sel_labels, icda, scheme=\"physician\")\n",
    "static_rem_states = suggest_term_by_scheme(sel_states, sel_labels, icda, scheme=\"static\")\n",
    "dynamic_rem_states = suggest_term_by_scheme(sel_states, sel_labels, icda, scheme=\"dynamic\")\n",
    "\n",
    "# Calculate model confidence for y_d\n",
    "random_cnfds_l = calc_cnfds_l_from_states(obs_states, rem_states=random_rem_states, dx_model=dx_model, tokenizer=ner_tokenizer, labels=[dx_classifier.dx2id[dx] for dx in sel_labels])\n",
    "physician_cnfds_l = calc_cnfds_l_from_states(obs_states, rem_states=physician_rem_states, dx_model=dx_model, tokenizer=ner_tokenizer, labels=[dx_classifier.dx2id[dx] for dx in sel_labels])\n",
    "static_cnfds_l = calc_cnfds_l_from_states(obs_states, rem_states=static_rem_states, dx_model=dx_model, tokenizer=ner_tokenizer, labels=[dx_classifier.dx2id[dx] for dx in sel_labels])\n",
    "dynamic_cnfds_l = calc_cnfds_l_from_states(obs_states, rem_states=dynamic_rem_states, dx_model=dx_model, tokenizer=ner_tokenizer, labels=[dx_classifier.dx2id[dx] for dx in sel_labels])\n",
    "\n",
    "# Calculate turns\n",
    "th_highs = [0.5, 0.7, 0.9]\n",
    "\n",
    "turns_by_approach = dict()\n",
    "\n",
    "random_term_usage = calc_term_usage(obs_states, cnfds_l=random_cnfds_l, th_highs=th_highs)\n",
    "physician_term_usage = calc_term_usage(obs_states, cnfds_l=physician_cnfds_l, th_highs=th_highs)\n",
    "static_term_usage = calc_term_usage(obs_states, cnfds_l=static_cnfds_l, th_highs=th_highs)\n",
    "dynamic_term_usage = calc_term_usage(obs_states, cnfds_l=dynamic_cnfds_l, th_highs=th_highs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cuda-11.3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35b66dd0c8f752918e1728d86abaa8fb004a7dee1d90779ea4d0023d852f9fe7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
